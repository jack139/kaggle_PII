{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9c4a86",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-29T11:06:45.614087Z",
     "iopub.status.busy": "2024-01-29T11:06:45.613751Z",
     "iopub.status.idle": "2024-01-29T11:06:45.629482Z",
     "shell.execute_reply": "2024-01-29T11:06:45.628364Z"
    },
    "papermill": {
     "duration": 0.024741,
     "end_time": "2024-01-29T11:06:45.631599",
     "exception": false,
     "start_time": "2024-01-29T11:06:45.606858",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bert4keras-packages/packs.tgz\n",
      "/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/pii_gp_best_f1_0.92609.h5\n",
      "/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/bert_config.json\n",
      "/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/vocab.txt\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/train.json\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/test.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "#import numpy as np # linear algebra\n",
    "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528159ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:06:45.644853Z",
     "iopub.status.busy": "2024-01-29T11:06:45.644590Z",
     "iopub.status.idle": "2024-01-29T11:06:46.617634Z",
     "shell.execute_reply": "2024-01-29T11:06:46.616565Z"
    },
    "papermill": {
     "duration": 0.981231,
     "end_time": "2024-01-29T11:06:46.619743",
     "exception": false,
     "start_time": "2024-01-29T11:06:45.638512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 29 11:06:46 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   36C    P0              25W / 250W |      0MiB / 16384MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24418b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:06:46.631929Z",
     "iopub.status.busy": "2024-01-29T11:06:46.631607Z",
     "iopub.status.idle": "2024-01-29T11:06:54.840635Z",
     "shell.execute_reply": "2024-01-29T11:06:54.839465Z"
    },
    "papermill": {
     "duration": 8.218786,
     "end_time": "2024-01-29T11:06:54.843908",
     "exception": false,
     "start_time": "2024-01-29T11:06:46.625122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages/\r\n",
      "packages/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\r\n",
      "packages/tensorboard_plugin_wit-1.8.1-py3-none-any.whl\r\n",
      "packages/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl\r\n",
      "packages/keras-2.8.0-py2.py3-none-any.whl\r\n",
      "packages/tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl\r\n",
      "packages/tensorboard-2.8.0-py3-none-any.whl\r\n",
      "packages/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl\r\n",
      "packages/bert4keras-0.11.5.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!tar xvfz /kaggle/input/bert4keras-packages/packs.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f77462c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:06:54.874293Z",
     "iopub.status.busy": "2024-01-29T11:06:54.873927Z",
     "iopub.status.idle": "2024-01-29T11:07:38.983105Z",
     "shell.execute_reply": "2024-01-29T11:07:38.981988Z"
    },
    "papermill": {
     "duration": 44.128779,
     "end_time": "2024-01-29T11:07:38.985672",
     "exception": false,
     "start_time": "2024-01-29T11:06:54.856893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/working/packages/\r\n",
      "Processing ./packages/bert4keras-0.11.5.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: bert4keras\r\n",
      "  Building wheel for bert4keras (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bert4keras: filename=bert4keras-0.11.5-py3-none-any.whl size=52070 sha256=9813de77d064d7fa58cf5df2947f308537df1b1e1b83d6b5683e35078cc1655e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/52/56/1d8ef6ed93c22424276d7f9c61f8e50feb9fd279131fd111e9\r\n",
      "Successfully built bert4keras\r\n",
      "Installing collected packages: bert4keras\r\n",
      "Successfully installed bert4keras-0.11.5\r\n",
      "Looking in links: file:///kaggle/working/packages/\r\n",
      "Processing ./packages/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl\r\n",
      "Processing ./packages/tensorboard-2.8.0-py3-none-any.whl\r\n",
      "Processing ./packages/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\r\n",
      "Processing ./packages/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl\r\n",
      "Processing ./packages/tensorboard_plugin_wit-1.8.1-py3-none-any.whl\r\n",
      "Installing collected packages: tf_estimator_nightly, tensorboard_plugin_wit, Keras_Preprocessing, tensorboard_data_server, tensorboard\r\n",
      "  Attempting uninstall: tensorboard_data_server\r\n",
      "    Found existing installation: tensorboard-data-server 0.7.1\r\n",
      "    Uninstalling tensorboard-data-server-0.7.1:\r\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.1\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.13.0\r\n",
      "    Uninstalling tensorboard-2.13.0:\r\n",
      "      Successfully uninstalled tensorboard-2.13.0\r\n",
      "Successfully installed Keras_Preprocessing-1.1.2 tensorboard-2.8.0 tensorboard_data_server-0.6.1 tensorboard_plugin_wit-1.8.1 tf_estimator_nightly-2.8.0.dev2021122109\r\n",
      "Looking in links: file:///kaggle/working/packages/\r\n",
      "Processing ./packages/tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl\r\n",
      "Processing ./packages/keras-2.8.0-py2.py3-none-any.whl\r\n",
      "Installing collected packages: tensorflow, keras\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.13.0\r\n",
      "    Uninstalling tensorflow-2.13.0:\r\n",
      "      Successfully uninstalled tensorflow-2.13.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.13.1\r\n",
      "    Uninstalling keras-2.13.1:\r\n",
      "      Successfully uninstalled keras-2.13.1\r\n",
      "Successfully installed keras-2.8.0 tensorflow-2.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert4keras --no-deps --force --no-index --find-links=file:///kaggle/working/packages/\n",
    "!pip install tensorboard_data_server tensorboard Keras_Preprocessing tf_estimator_nightly tensorboard_plugin_wit --no-deps --force --no-index --find-links=file:///kaggle/working/packages/\n",
    "!pip install tensorflow keras --no-deps --force --no-index --find-links=file:///kaggle/working/packages/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c8876",
   "metadata": {
    "papermill": {
     "duration": 0.007417,
     "end_time": "2024-01-29T11:07:39.001697",
     "exception": false,
     "start_time": "2024-01-29T11:07:38.994280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initial env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dd9def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:07:39.018541Z",
     "iopub.status.busy": "2024-01-29T11:07:39.018229Z",
     "iopub.status.idle": "2024-01-29T11:07:39.022334Z",
     "shell.execute_reply": "2024-01-29T11:07:39.021523Z"
    },
    "papermill": {
     "duration": 0.014734,
     "end_time": "2024-01-29T11:07:39.024105",
     "exception": false,
     "start_time": "2024-01-29T11:07:39.009371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install bert4keras --no-deps --force\n",
    "#!pip install tensorflow-gpu==2.8.0\n",
    "#!pip install tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bb6133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:07:39.040739Z",
     "iopub.status.busy": "2024-01-29T11:07:39.040486Z",
     "iopub.status.idle": "2024-01-29T11:07:44.847156Z",
     "shell.execute_reply": "2024-01-29T11:07:44.846080Z"
    },
    "papermill": {
     "duration": 5.817828,
     "end_time": "2024-01-29T11:07:44.849705",
     "exception": false,
     "start_time": "2024-01-29T11:07:39.031877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /opt/conda/lib/python3.10/site-packages/bert4keras/backend.py _b.py\n",
    "!sed -i \"18s/sys/#sys/\" _b.py\n",
    "!sed \"18a\\    from tensorflow import keras\\n    import tensorflow.keras.backend as K\" _b.py > _b2.py\n",
    "!sed -i \"22s/import/#import/\" _b2.py\n",
    "!sed -i \"23s/import/#import/\" _b2.py\n",
    "!cp _b2.py /opt/conda/lib/python3.10/site-packages/bert4keras/backend.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f878b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:07:44.867325Z",
     "iopub.status.busy": "2024-01-29T11:07:44.867009Z",
     "iopub.status.idle": "2024-01-29T11:07:44.871423Z",
     "shell.execute_reply": "2024-01-29T11:07:44.870620Z"
    },
    "papermill": {
     "duration": 0.015307,
     "end_time": "2024-01-29T11:07:44.873302",
     "exception": false,
     "start_time": "2024-01-29T11:07:44.857995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!curl -LJO https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip\n",
    "#!unzip uncased_L-12_H-768_A-12.zip -d bert_base\n",
    "\n",
    "#!curl -LJO https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "#!unzip uncased_L-12_H-768_A-12.zip -d bert_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd0adc",
   "metadata": {
    "papermill": {
     "duration": 0.007646,
     "end_time": "2024-01-29T11:07:44.888687",
     "exception": false,
     "start_time": "2024-01-29T11:07:44.881041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convert raw data to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2064f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:07:44.905569Z",
     "iopub.status.busy": "2024-01-29T11:07:44.905281Z",
     "iopub.status.idle": "2024-01-29T11:07:45.013833Z",
     "shell.execute_reply": "2024-01-29T11:07:45.012889Z"
    },
    "papermill": {
     "duration": 0.119466,
     "end_time": "2024-01-29T11:07:45.015853",
     "exception": false,
     "start_time": "2024-01-29T11:07:44.896387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "train_file = '/kaggle/input/pii-detection-removal-from-educational-data/train.json'\n",
    "test_file = '/kaggle/input/pii-detection-removal-from-educational-data/test.json'\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "\n",
    "def __convert(indata, include_blank=True):\n",
    "\n",
    "    text = ''\n",
    "    entities = []\n",
    "    all_idx = 0\n",
    "    start_idx = 0\n",
    "    etype = ''\n",
    "\n",
    "    text = indata['sentence']\n",
    "\n",
    "    for n, label in enumerate(indata['BIO_label']):\n",
    "        if label[0]=='O':\n",
    "            if etype!='':\n",
    "                entities.append({\n",
    "                    \"start_idx\": len(''.join(text[:start_idx])),\n",
    "                    \"end_idx\": len((''.join(text[:all_idx])).rstrip()) - 1,\n",
    "                    \"type\": etype,\n",
    "                    \"entity\": (''.join(text[start_idx:all_idx])).rstrip(),\n",
    "                })\n",
    "            start_idx = 0\n",
    "            etype = ''\n",
    "        elif label[0]=='B':\n",
    "            if etype!='':\n",
    "                entities.append({\n",
    "                    \"start_idx\": len(''.join(text[:start_idx])),\n",
    "                    \"end_idx\": len((''.join(text[:all_idx])).rstrip()) - 1,\n",
    "                    \"type\": etype,\n",
    "                    \"entity\": (''.join(text[start_idx:all_idx])).rstrip(),\n",
    "                })                \n",
    "            start_idx = all_idx\n",
    "            etype = label.split('-')[1]\n",
    "        elif label[0]=='I':\n",
    "            pass\n",
    "        else:\n",
    "            print('unknown label: ', label)\n",
    "\n",
    "        all_idx += 1\n",
    "\n",
    "\n",
    "    # 一行text结束\n",
    "    if etype!='':\n",
    "        entities.append({\n",
    "            \"start_idx\": len(''.join(text[:start_idx])),\n",
    "            \"end_idx\": len((''.join(text[:all_idx])).rstrip()) - 1,\n",
    "            \"type\": etype,\n",
    "            \"entity\": (''.join(text[start_idx:all_idx])).rstrip(),\n",
    "        })\n",
    "\n",
    "    # 加入数据集\n",
    "    if include_blank or len(entities)>0:\n",
    "        return {\n",
    "            'text' : ''.join(text),\n",
    "            'entities' : entities,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def assemble(infile, outfile_path, max_len=500, is_train=True, include_blank=True):\n",
    "    total = text_break = tmp_break = 0\n",
    "    D = []\n",
    "\n",
    "    data = json.load(open(infile))\n",
    "    for l in tqdm(data):\n",
    "        #print(f\"---> {l['document']}\")\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        text = []\n",
    "        tmp_text = []\n",
    "        n = n_text = n_tmp = 0\n",
    "        while n<len(l['tokens']):\n",
    "            token = l['tokens'][n].replace('…', '.').replace('´', \"'\").replace('²', '2')\\\n",
    "                .replace('΅', \"'\").replace('¨', \"'\").replace(';', ';').replace('．', '.')\\\n",
    "                .replace('³', '3').replace('‑', '-').replace('¹', '1').replace('½', '1')\\\n",
    "                .replace('¾', '1').replace('¼', '1').replace('\\xad', '-')\\\n",
    "                .replace('ﬄ', 'ffl').replace('ﬃ', 'ffi').replace('ﬂ', 'fl')\\\n",
    "                .replace('ﬁ', 'fi').replace('ﬀ', 'ff').replace('™', 'TM').replace('№', 'No')\n",
    "\n",
    "            if l['trailing_whitespace'][n]:\n",
    "                token += ' ' \n",
    "\n",
    "            if n_tmp + 1 > max_len:\n",
    "                text += deepcopy(tmp_text)\n",
    "                tmp_text = []\n",
    "                n_text += n_tmp\n",
    "                n_tmp = 0\n",
    "\n",
    "                tmp_break += 1\n",
    "\n",
    "                #print(text)\n",
    "                #assert False, f\"tmp_text is too long: {len(text)}, {len(tmp_text)}, {len(token)}\"\n",
    "\n",
    "            if n_text + n_tmp + 1 > max_len:\n",
    "                assert n_text>0, f\"too long: {n_text}, {n_tmp}, {max_len}\"\n",
    "                #print(text)\n",
    "                #print('-'*20)\n",
    "                dd = __convert({\n",
    "                        'sentence'  : [x[0] for x in text],\n",
    "                        'BIO_label' : [x[1] for x in text],\n",
    "                    }, include_blank=include_blank)\n",
    "                if dd:\n",
    "                    dd['document'] = l['document']\n",
    "                    if not is_train:\n",
    "                        dd['tokens'] = [x[0] for x in text]\n",
    "                    D.append(dd)\n",
    "                text = []\n",
    "                n_text = 0\n",
    "\n",
    "                text_break += 1\n",
    "\n",
    "            if is_train:\n",
    "                tmp_text += [(token, l['labels'][n])] # token, label\n",
    "            else:\n",
    "                tmp_text += [(token, 'O')] # token, blank-label\n",
    "            n_tmp += 1\n",
    "\n",
    "            if token=='\\n\\n':\n",
    "                text += deepcopy(tmp_text)\n",
    "                tmp_text = []\n",
    "                n_text += n_tmp\n",
    "                n_tmp = 0\n",
    "\n",
    "            n += 1\n",
    "\n",
    "        if n_text + n_tmp > 0:\n",
    "            text += deepcopy(tmp_text)\n",
    "            n_text += n_tmp\n",
    "            #print(text)\n",
    "            #print('-'*20)\n",
    "            dd = __convert({\n",
    "                    'sentence'  : [x[0] for x in text],\n",
    "                    'BIO_label' : [x[1] for x in text],\n",
    "                }, include_blank=include_blank)\n",
    "            if dd:\n",
    "                dd['document'] = l['document']\n",
    "                if not is_train:\n",
    "                    dd['tokens'] = [x[0] for x in text]\n",
    "                D.append(dd)\n",
    "\n",
    "            text_break += 1            \n",
    "\n",
    "        #break # for test\n",
    "\n",
    "    blank = 0\n",
    "    for x in D:\n",
    "        if len(x['entities'])==0:\n",
    "            blank += 1\n",
    "\n",
    "    print(f\"total= {total}\\ttext_break= {text_break}\\ttmp_break= {tmp_break}\\tblank= {blank}\")\n",
    "\n",
    "    if is_train:\n",
    "        random.shuffle(D)\n",
    "\n",
    "        # 拆分数据集\n",
    "        split_n = int(len(D) * split_ratio)\n",
    "\n",
    "        json.dump(\n",
    "            D[:split_n],\n",
    "            open(os.path.join(outfile_path, \"train.json\"), 'w', encoding='utf-8'),\n",
    "            indent=4,\n",
    "            ensure_ascii=False\n",
    "        )\n",
    "\n",
    "\n",
    "        json.dump(\n",
    "            D[split_n:],\n",
    "            open(os.path.join(outfile_path, \"dev.json\"), 'w', encoding='utf-8'),\n",
    "            indent=4,\n",
    "            ensure_ascii=False\n",
    "        )\n",
    "\n",
    "        print(f\"train set: {split_n}\\tdev set: {len(D)-split_n}\")\n",
    "\n",
    "    else:\n",
    "        json.dump(\n",
    "            D,\n",
    "            open(os.path.join(outfile_path, \"test.json\"), 'w', encoding='utf-8'),\n",
    "            indent=4,\n",
    "            ensure_ascii=False\n",
    "        )\n",
    "\n",
    "        print(f\"test set: {len(D)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cde660b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:07:45.032560Z",
     "iopub.status.busy": "2024-01-29T11:07:45.032274Z",
     "iopub.status.idle": "2024-01-29T11:08:27.044549Z",
     "shell.execute_reply": "2024-01-29T11:08:27.043527Z"
    },
    "papermill": {
     "duration": 42.023072,
     "end_time": "2024-01-29T11:08:27.046716",
     "exception": false,
     "start_time": "2024-01-29T11:07:45.023644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6807/6807 [00:39<00:00, 173.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total= 6807\ttext_break= 14370\ttmp_break= 829\tblank= 0\n",
      "train set: 894\tdev set: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 150.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total= 10\ttext_break= 24\ttmp_break= 3\tblank= 24\n",
      "test set: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assemble(train_file, '.', include_blank=False)\n",
    "assemble(test_file, '.', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39804fdc",
   "metadata": {
    "papermill": {
     "duration": 0.035815,
     "end_time": "2024-01-29T11:08:27.118591",
     "exception": false,
     "start_time": "2024-01-29T11:08:27.082776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19977dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:08:27.192331Z",
     "iopub.status.busy": "2024-01-29T11:08:27.192006Z",
     "iopub.status.idle": "2024-01-29T11:08:38.261860Z",
     "shell.execute_reply": "2024-01-29T11:08:38.260789Z"
    },
    "papermill": {
     "duration": 11.297562,
     "end_time": "2024-01-29T11:08:38.451835",
     "exception": false,
     "start_time": "2024-01-29T11:08:27.154273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['EMAIL', 'ID_NUM', 'NAME_STUDENT', 'PHONE_NUM', 'STREET_ADDRESS', 'URL_PERSONAL', 'USERNAME']\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (Embedding)    (None, None, 1024)   31254528    ['Input-Token[0][0]']            \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, None, 1024)   2048        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, None, 1024)   0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, None, 1024)  524288      ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, None, 1024)  2048        ['Embedding-Position[0][0]']     \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, None, 1024)   0           ['Embedding-Norm[0][0]']         \n",
      "                                                                                                  \n",
      " Transformer-0-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Embedding-Dropout[0][0]',      \n",
      " ention (MultiHeadAttention)                                      'Embedding-Dropout[0][0]',      \n",
      "                                                                  'Embedding-Dropout[0][0]']      \n",
      "                                                                                                  \n",
      " Transformer-0-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-0-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-0-MultiHeadSelfAtt  (None, None, 1024)  0           ['Embedding-Dropout[0][0]',      \n",
      " ention-Add (Add)                                                 'Transformer-0-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-0-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-0-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-0-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-0-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-0-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-0-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-0-FeedForward-Add   (None, None, 1024)  0           ['Transformer-0-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-0-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-0-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-0-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-1-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-0-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-0-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-0-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-1-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-1-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-1-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-0-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-1-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-1-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-1-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-1-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-1-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-1-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-1-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-1-FeedForward-Add   (None, None, 1024)  0           ['Transformer-1-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-1-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-1-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-1-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-2-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-1-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-1-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-1-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-2-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-2-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-2-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-1-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-2-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-2-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-2-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-2-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-2-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-2-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-2-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-2-FeedForward-Add   (None, None, 1024)  0           ['Transformer-2-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-2-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-2-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-2-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-3-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-2-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-2-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-2-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-3-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-3-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-3-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-2-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-3-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-3-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-3-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-3-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-3-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-3-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-3-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-3-FeedForward-Add   (None, None, 1024)  0           ['Transformer-3-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-3-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-3-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-3-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-4-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-3-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-3-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-3-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-4-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-4-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-4-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-3-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-4-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-4-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-4-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-4-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-4-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-4-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-4-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-4-FeedForward-Add   (None, None, 1024)  0           ['Transformer-4-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-4-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-4-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-4-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-5-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-4-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-4-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-4-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-5-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-5-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-5-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-4-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-5-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-5-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-5-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-5-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-5-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-5-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-5-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-5-FeedForward-Add   (None, None, 1024)  0           ['Transformer-5-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-5-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-5-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-5-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-6-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-5-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-5-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-5-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-6-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-6-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-6-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-5-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-6-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-6-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-6-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-6-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-6-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-6-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-6-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-6-FeedForward-Add   (None, None, 1024)  0           ['Transformer-6-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-6-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-6-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-6-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-7-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-6-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-6-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-6-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-7-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-7-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-7-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-6-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-7-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-7-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-7-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-7-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-7-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-7-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-7-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-7-FeedForward-Add   (None, None, 1024)  0           ['Transformer-7-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-7-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-7-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-7-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-8-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-7-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-7-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-7-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-8-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-8-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-8-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-7-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-8-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-8-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-8-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-8-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-8-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-8-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-8-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-8-FeedForward-Add   (None, None, 1024)  0           ['Transformer-8-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-8-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-8-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-8-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-9-MultiHeadSelfAtt  (None, None, 1024)  4198400     ['Transformer-8-FeedForward-Norm[\n",
      " ention (MultiHeadAttention)                                     0][0]',                          \n",
      "                                                                  'Transformer-8-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-8-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-9-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-9-MultiHeadSelfAtte\n",
      " ention-Dropout (Dropout)                                        ntion[0][0]']                    \n",
      "                                                                                                  \n",
      " Transformer-9-MultiHeadSelfAtt  (None, None, 1024)  0           ['Transformer-8-FeedForward-Norm[\n",
      " ention-Add (Add)                                                0][0]',                          \n",
      "                                                                  'Transformer-9-MultiHeadSelfAtte\n",
      "                                                                 ntion-Dropout[0][0]']            \n",
      "                                                                                                  \n",
      " Transformer-9-MultiHeadSelfAtt  (None, None, 1024)  2048        ['Transformer-9-MultiHeadSelfAtte\n",
      " ention-Norm (LayerNormalizatio                                  ntion-Add[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " Transformer-9-FeedForward (Fee  (None, None, 1024)  8393728     ['Transformer-9-MultiHeadSelfAtte\n",
      " dForward)                                                       ntion-Norm[0][0]']               \n",
      "                                                                                                  \n",
      " Transformer-9-FeedForward-Drop  (None, None, 1024)  0           ['Transformer-9-FeedForward[0][0]\n",
      " out (Dropout)                                                   ']                               \n",
      "                                                                                                  \n",
      " Transformer-9-FeedForward-Add   (None, None, 1024)  0           ['Transformer-9-MultiHeadSelfAtte\n",
      " (Add)                                                           ntion-Norm[0][0]',               \n",
      "                                                                  'Transformer-9-FeedForward-Dropo\n",
      "                                                                 ut[0][0]']                       \n",
      "                                                                                                  \n",
      " Transformer-9-FeedForward-Norm  (None, None, 1024)  2048        ['Transformer-9-FeedForward-Add[0\n",
      "  (LayerNormalization)                                           ][0]']                           \n",
      "                                                                                                  \n",
      " Transformer-10-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-9-FeedForward-Norm[\n",
      " tention (MultiHeadAttention)                                    0][0]',                          \n",
      "                                                                  'Transformer-9-FeedForward-Norm[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'Transformer-9-FeedForward-Norm[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-10-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-10-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-10-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-9-FeedForward-Norm[\n",
      " tention-Add (Add)                                               0][0]',                          \n",
      "                                                                  'Transformer-10-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-10-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-10-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-10-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-10-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-10-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-10-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-10-FeedForward-Add  (None, None, 1024)  0           ['Transformer-10-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-10-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-10-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-10-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-11-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-10-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-10-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-10-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-11-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-11-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-11-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-10-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-11-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-11-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-11-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-11-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-11-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-11-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-11-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-11-FeedForward-Add  (None, None, 1024)  0           ['Transformer-11-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-11-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-11-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-11-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-12-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-11-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-11-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-11-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-12-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-12-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-12-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-11-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-12-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-12-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-12-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-12-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-12-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-12-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-12-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-12-FeedForward-Add  (None, None, 1024)  0           ['Transformer-12-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-12-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-12-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-12-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-13-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-12-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-12-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-12-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-13-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-13-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-13-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-12-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-13-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-13-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-13-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-13-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-13-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-13-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-13-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-13-FeedForward-Add  (None, None, 1024)  0           ['Transformer-13-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-13-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-13-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-13-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-14-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-13-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-13-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-13-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-14-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-14-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-14-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-13-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-14-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-14-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-14-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-14-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-14-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-14-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-14-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-14-FeedForward-Add  (None, None, 1024)  0           ['Transformer-14-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-14-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-14-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-14-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-15-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-14-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-14-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-14-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-15-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-15-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-15-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-14-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-15-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-15-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-15-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-15-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-15-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-15-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-15-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-15-FeedForward-Add  (None, None, 1024)  0           ['Transformer-15-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-15-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-15-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-15-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-16-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-15-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-15-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-15-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-16-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-16-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-16-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-15-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-16-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-16-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-16-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-16-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-16-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-16-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-16-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-16-FeedForward-Add  (None, None, 1024)  0           ['Transformer-16-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-16-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-16-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-16-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-17-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-16-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-16-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-16-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-17-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-17-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-17-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-16-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-17-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-17-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-17-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-17-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-17-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-17-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-17-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-17-FeedForward-Add  (None, None, 1024)  0           ['Transformer-17-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-17-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-17-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-17-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-18-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-17-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-17-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-17-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-18-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-18-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-18-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-17-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-18-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-18-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-18-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-18-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-18-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-18-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-18-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-18-FeedForward-Add  (None, None, 1024)  0           ['Transformer-18-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-18-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-18-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-18-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-19-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-18-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-18-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-18-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-19-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-19-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-19-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-18-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-19-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-19-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-19-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-19-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-19-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-19-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-19-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-19-FeedForward-Add  (None, None, 1024)  0           ['Transformer-19-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-19-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-19-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-19-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-20-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-19-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-19-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-19-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-20-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-20-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-20-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-19-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-20-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-20-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-20-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-20-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-20-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-20-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-20-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-20-FeedForward-Add  (None, None, 1024)  0           ['Transformer-20-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-20-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-20-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-20-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-21-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-20-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-20-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-20-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-21-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-21-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-21-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-20-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-21-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-21-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-21-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-21-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-21-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-21-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-21-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-21-FeedForward-Add  (None, None, 1024)  0           ['Transformer-21-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-21-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-21-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-21-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-22-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-21-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-21-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-21-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-22-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-22-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-22-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-21-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-22-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-22-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-22-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-22-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-22-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-22-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-22-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-22-FeedForward-Add  (None, None, 1024)  0           ['Transformer-22-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-22-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-22-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-22-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " Transformer-23-MultiHeadSelfAt  (None, None, 1024)  4198400     ['Transformer-22-FeedForward-Norm\n",
      " tention (MultiHeadAttention)                                    [0][0]',                         \n",
      "                                                                  'Transformer-22-FeedForward-Norm\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'Transformer-22-FeedForward-Norm\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " Transformer-23-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-23-MultiHeadSelfAtt\n",
      " tention-Dropout (Dropout)                                       ention[0][0]']                   \n",
      "                                                                                                  \n",
      " Transformer-23-MultiHeadSelfAt  (None, None, 1024)  0           ['Transformer-22-FeedForward-Norm\n",
      " tention-Add (Add)                                               [0][0]',                         \n",
      "                                                                  'Transformer-23-MultiHeadSelfAtt\n",
      "                                                                 ention-Dropout[0][0]']           \n",
      "                                                                                                  \n",
      " Transformer-23-MultiHeadSelfAt  (None, None, 1024)  2048        ['Transformer-23-MultiHeadSelfAtt\n",
      " tention-Norm (LayerNormalizati                                  ention-Add[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Transformer-23-FeedForward (Fe  (None, None, 1024)  8393728     ['Transformer-23-MultiHeadSelfAtt\n",
      " edForward)                                                      ention-Norm[0][0]']              \n",
      "                                                                                                  \n",
      " Transformer-23-FeedForward-Dro  (None, None, 1024)  0           ['Transformer-23-FeedForward[0][0\n",
      " pout (Dropout)                                                  ]']                              \n",
      "                                                                                                  \n",
      " Transformer-23-FeedForward-Add  (None, None, 1024)  0           ['Transformer-23-MultiHeadSelfAtt\n",
      "  (Add)                                                          ention-Norm[0][0]',              \n",
      "                                                                  'Transformer-23-FeedForward-Drop\n",
      "                                                                 out[0][0]']                      \n",
      "                                                                                                  \n",
      " Transformer-23-FeedForward-Nor  (None, None, 1024)  2048        ['Transformer-23-FeedForward-Add[\n",
      " m (LayerNormalization)                                          0][0]']                          \n",
      "                                                                                                  \n",
      " efficient_global_pointer (Effi  (None, 7, None, Non  133006     ['Transformer-23-FeedForward-Norm\n",
      " cientGlobalPointer)            e)                               [0][0]']                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 334,225,294\n",
      "Trainable params: 334,225,294\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "# 用GlobalPointer做中文命名实体识别\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_KERAS\"] = \"1\" # use tf 2.7 keras\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.backend import multilabel_categorical_crossentropy\n",
    "#from bert4keras.layers import GlobalPointer\n",
    "from bert4keras.layers import EfficientGlobalPointer as GlobalPointer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open, to_array\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "maxlen = 512\n",
    "epochs = 10\n",
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "categories = set()\n",
    "\n",
    "# bert配置\n",
    "config_path = '/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/bert_config.json'\n",
    "checkpoint_path = None #'/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/bert_model.ckpt'\n",
    "dict_path = '/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/vocab.txt'\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：[text, (start, end, label), (start, end, label), ...]，\n",
    "              意味着text[start:end + 1]是类型为label的实体。\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    for d in json.load(open(filename)):\n",
    "        D.append([d['text']])\n",
    "        for e in d['entities']:\n",
    "            start, end, label = e['start_idx'], e['end_idx'], e['type']\n",
    "            if start <= end:\n",
    "                D[-1].append((start, end, label))\n",
    "            categories.add(label)\n",
    "    return D\n",
    "\n",
    "\n",
    "# 标注数据\n",
    "train_data = load_data('./train.json')\n",
    "valid_data = load_data('./dev.json')\n",
    "categories = list(sorted(categories))\n",
    "\n",
    "print(\"labels: \", categories)\n",
    "# labels:  ['EMAIL', 'ID_NUM', 'NAME_STUDENT', 'PHONE_NUM', 'STREET_ADDRESS', 'URL_PERSONAL', 'USERNAME']\n",
    "\n",
    "\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for is_end, d in self.sample(random):\n",
    "            tokens = tokenizer.tokenize(d[0], maxlen=maxlen)\n",
    "            mapping = tokenizer.rematch(d[0], tokens)\n",
    "            start_mapping = {j[0]: i for i, j in enumerate(mapping) if j}\n",
    "            end_mapping = {j[-1]: i for i, j in enumerate(mapping) if j}\n",
    "            token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "            segment_ids = [0] * len(token_ids)\n",
    "            labels = np.zeros((len(categories), maxlen, maxlen))\n",
    "            for start, end, label in d[1:]:\n",
    "                if start in start_mapping and end in end_mapping:\n",
    "                    start = start_mapping[start]\n",
    "                    end = end_mapping[end]\n",
    "                    label = categories.index(label)\n",
    "                    labels[label, start, end] = 1\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append(labels[:, :len(token_ids), :len(token_ids)])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels, seq_dims=3)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "\n",
    "\n",
    "def global_pointer_crossentropy(y_true, y_pred):\n",
    "    \"\"\"给GlobalPointer设计的交叉熵\n",
    "    \"\"\"\n",
    "    bh = K.prod(K.shape(y_pred)[:2])\n",
    "    y_true = K.reshape(y_true, (bh, -1))\n",
    "    y_pred = K.reshape(y_pred, (bh, -1))\n",
    "    return K.mean(multilabel_categorical_crossentropy(y_true, y_pred))\n",
    "\n",
    "\n",
    "def global_pointer_f1_score(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"给GlobalPointer设计的F1\n",
    "    \"\"\"\n",
    "    y_pred = K.cast(K.greater(y_pred, 0), K.floatx())\n",
    "    return 2 * K.sum(y_true * y_pred) / (K.sum(y_true + y_pred) + epsilon)\n",
    "\n",
    "\n",
    "model = build_transformer_model(config_path, checkpoint_path)\n",
    "output = GlobalPointer(len(categories), 64)(model.output)\n",
    "\n",
    "AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\n",
    "optimizer = AdamEMA(learning_rate=learning_rate)\n",
    "\n",
    "model = Model(model.input, output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=global_pointer_crossentropy,\n",
    "    optimizer=optimizer, #Adam(learning_rate),\n",
    "    #optimizer=Adam(learning_rate),\n",
    "    metrics=[global_pointer_f1_score]\n",
    ")\n",
    "\n",
    "\n",
    "class NamedEntityRecognizer(object):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text, threshold=0):\n",
    "        tokens = tokenizer.tokenize(text, maxlen=512)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "        scores = model.predict([token_ids, segment_ids])[0]\n",
    "        scores[:, [0, -1]] -= np.inf\n",
    "        scores[:, :, [0, -1]] -= np.inf\n",
    "        entities = []\n",
    "        for l, start, end in zip(*np.where(scores > threshold)):\n",
    "            entities.append(\n",
    "                (mapping[start][0], mapping[end][-1], categories[l])\n",
    "            )\n",
    "        return entities\n",
    "\n",
    "\n",
    "NER = NamedEntityRecognizer()\n",
    "\n",
    "\n",
    "def evaluate(data):\n",
    "    \"\"\"评测函数\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    for d in tqdm(data, ncols=100):\n",
    "        R = set(NER.recognize(d[0]))\n",
    "        T = set([tuple(i) for i in d[1:]])\n",
    "        X += len(R & T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "    f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_val_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        f1, precision, recall = evaluate(valid_data)\n",
    "        # 保存最优\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            model.save_weights('./pii_gp_best_f1.h5')\n",
    "        print(\n",
    "            'valid:  f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "            (f1, precision, recall, self.best_val_f1)\n",
    "        )\n",
    "\n",
    "\n",
    "def predict_to_file(in_file, out_file):\n",
    "    \"\"\"预测到文件\n",
    "    \"\"\"\n",
    "    data = json.load(open(in_file))\n",
    "\n",
    "    document = \"\"\n",
    "    last_pos = 0\n",
    "    D = []\n",
    "\n",
    "    for d in tqdm(data, ncols=100):\n",
    "\n",
    "        if document != d['document']:\n",
    "            document = d['document']\n",
    "            last_pos = 0\n",
    "\n",
    "        # 初始化 BIO 标记\n",
    "        label = ['O']*len(d['tokens'])\n",
    "\n",
    "        # 识别\n",
    "        entities = NER.recognize(d['text'])\n",
    "        for e in entities:\n",
    "            d['entities'].append({\n",
    "                'start_idx': e[0],\n",
    "                'end_idx': e[1],\n",
    "                'type': e[2]\n",
    "            })\n",
    "\n",
    "            # 生成 BIO标记, 依据 原始 tokens\n",
    "            pos = last = 0\n",
    "            for n, x in enumerate(d['tokens']):\n",
    "                if pos >= e[0] and pos <= e[1]+1:\n",
    "                    if last==0:\n",
    "                        label[n] = 'B-'+e[2]\n",
    "                        last += 1\n",
    "                    else:\n",
    "                        label[n] = 'I-'+e[2]\n",
    "                    D.append((document, last_pos+n, label[n]))\n",
    "                else:\n",
    "                    last = 0\n",
    "\n",
    "                pos += len(x)\n",
    "\n",
    "                if pos > e[1]:\n",
    "                    break\n",
    "\n",
    "        d['labels'] = label\n",
    "\n",
    "        last_pos += len(d['tokens'])\n",
    "\n",
    "    # 保存json格式\n",
    "    json.dump(\n",
    "        data,\n",
    "        open(\"output.json\", 'w', encoding='utf-8'),\n",
    "        indent=4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        f.write(\"row_id,document,token,label\\n\")\n",
    "        for n, x in enumerate(D):\n",
    "            f.write(f\"{n},{x[0]},{x[1]},{x[2]}\\n\")\n",
    "\n",
    "def lr_step_decay(epoch):\n",
    "    drop = 0.8\n",
    "    epochs_drop = 1.0\n",
    "    lrate = learning_rate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ced9e9",
   "metadata": {
    "papermill": {
     "duration": 0.1006,
     "end_time": "2024-01-29T11:08:38.694570",
     "exception": false,
     "start_time": "2024-01-29T11:08:38.593970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b112e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T11:08:38.893101Z",
     "iopub.status.busy": "2024-01-29T11:08:38.892736Z",
     "iopub.status.idle": "2024-01-29T12:51:18.506392Z",
     "shell.execute_reply": "2024-01-29T12:51:18.505464Z"
    },
    "papermill": {
     "duration": 6159.715868,
     "end_time": "2024-01-29T12:51:18.508279",
     "exception": false,
     "start_time": "2024-01-29T11:08:38.792411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 0.0257 - global_pointer_f1_score: 0.9849"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:54<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.97703, precision: 0.97256, recall: 0.98154, best f1: 0.97703\n",
      "\n",
      "447/447 [==============================] - 401s 723ms/step - loss: 0.0257 - global_pointer_f1_score: 0.9849 - lr: 8.0000e-06\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 0.0065 - global_pointer_f1_score: 0.9978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.97205, precision: 0.98119, recall: 0.96308, best f1: 0.97703\n",
      "\n",
      "447/447 [==============================] - 301s 675ms/step - loss: 0.0065 - global_pointer_f1_score: 0.9978 - lr: 6.4000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 0.0031 - global_pointer_f1_score: 0.9978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:36<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.97846, precision: 0.97846, recall: 0.97846, best f1: 0.97846\n",
      "\n",
      "447/447 [==============================] - 305s 682ms/step - loss: 0.0031 - global_pointer_f1_score: 0.9978 - lr: 5.1200e-06\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 0.0051 - global_pointer_f1_score: 0.9988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98154\n",
      "\n",
      "447/447 [==============================] - 305s 681ms/step - loss: 0.0051 - global_pointer_f1_score: 0.9988 - lr: 4.0960e-06\n",
      "Epoch 5/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 0.0026 - global_pointer_f1_score: 0.9980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98154\n",
      "\n",
      "447/447 [==============================] - 304s 679ms/step - loss: 0.0026 - global_pointer_f1_score: 0.9980 - lr: 3.2768e-06\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 6.3119e-04 - global_pointer_f1_score: 0.9999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98154\n",
      "\n",
      "447/447 [==============================] - 303s 677ms/step - loss: 6.3119e-04 - global_pointer_f1_score: 0.9999 - lr: 2.6214e-06\n",
      "Epoch 7/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 4.8689e-04 - global_pointer_f1_score: 0.9999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98154\n",
      "\n",
      "447/447 [==============================] - 304s 680ms/step - loss: 4.8689e-04 - global_pointer_f1_score: 0.9999 - lr: 2.0972e-06\n",
      "Epoch 8/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 6.1617e-04 - global_pointer_f1_score: 0.9999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98305, precision: 0.98457, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 304s 681ms/step - loss: 6.1617e-04 - global_pointer_f1_score: 0.9999 - lr: 1.6777e-06\n",
      "Epoch 9/20\n",
      "447/447 [==============================] - ETA: 0s - loss: 0.0021 - global_pointer_f1_score: 0.9997"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:37<00:00,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 303s 679ms/step - loss: 0.0021 - global_pointer_f1_score: 0.9997 - lr: 1.3422e-06\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 0.0035 - global_pointer_f1_score: 0.9997"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:36<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.97853, precision: 0.97554, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 303s 678ms/step - loss: 0.0035 - global_pointer_f1_score: 0.9997 - lr: 1.0737e-06\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 5.8249e-04 - global_pointer_f1_score: 0.9999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 301s 673ms/step - loss: 5.8249e-04 - global_pointer_f1_score: 0.9999 - lr: 8.5899e-07\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 0.0018 - global_pointer_f1_score: 0.9996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.97853, precision: 0.97554, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 302s 676ms/step - loss: 0.0018 - global_pointer_f1_score: 0.9996 - lr: 6.8719e-07\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 5.5872e-04 - global_pointer_f1_score: 0.9997"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 299s 669ms/step - loss: 5.5872e-04 - global_pointer_f1_score: 0.9997 - lr: 5.4976e-07\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 0.0018 - global_pointer_f1_score: 0.9989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 302s 675ms/step - loss: 0.0018 - global_pointer_f1_score: 0.9989 - lr: 4.3980e-07\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 3.0113e-04 - global_pointer_f1_score: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:36<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 301s 673ms/step - loss: 3.0113e-04 - global_pointer_f1_score: 1.0000 - lr: 3.5184e-07\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 0.0011 - global_pointer_f1_score: 0.9998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 302s 676ms/step - loss: 0.0011 - global_pointer_f1_score: 0.9998 - lr: 2.8147e-07\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 2.9376e-04 - global_pointer_f1_score: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98003, precision: 0.97853, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 301s 674ms/step - loss: 2.9376e-04 - global_pointer_f1_score: 1.0000 - lr: 2.2518e-07\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 2.8595e-04 - global_pointer_f1_score: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 303s 678ms/step - loss: 2.8595e-04 - global_pointer_f1_score: 1.0000 - lr: 1.8014e-07\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 0.0019 - global_pointer_f1_score: 0.9996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:35<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 301s 673ms/step - loss: 0.0019 - global_pointer_f1_score: 0.9996 - lr: 1.4412e-07\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - ETA: 0s - loss: 0.0018 - global_pointer_f1_score: 0.9993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 224/224 [00:36<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  f1: 0.98154, precision: 0.98154, recall: 0.98154, best f1: 0.98305\n",
      "\n",
      "447/447 [==============================] - 303s 678ms/step - loss: 0.0018 - global_pointer_f1_score: 0.9993 - lr: 1.1529e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f1c1bd000>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "batch_size = 2\n",
    "epochs = 20\n",
    "\n",
    "evaluator = Evaluator()\n",
    "lrate = LearningRateScheduler(lr_step_decay)\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "\n",
    "model.load_weights('/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/pii_gp_best_f1_0.92609.h5')\n",
    "\n",
    "model.fit(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    callbacks=[evaluator, lrate]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91282220",
   "metadata": {
    "papermill": {
     "duration": 1.177897,
     "end_time": "2024-01-29T12:51:20.914367",
     "exception": false,
     "start_time": "2024-01-29T12:51:19.736470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbae44f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T12:51:23.293688Z",
     "iopub.status.busy": "2024-01-29T12:51:23.293301Z",
     "iopub.status.idle": "2024-01-29T12:51:28.412690Z",
     "shell.execute_reply": "2024-01-29T12:51:28.411893Z"
    },
    "papermill": {
     "duration": 6.268356,
     "end_time": "2024-01-29T12:51:28.414763",
     "exception": false,
     "start_time": "2024-01-29T12:51:22.146407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# 推理生成结果\n",
    "\n",
    "model.load_weights('pii_gp_best_f1.h5')\n",
    "predict_to_file('test.json', 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4346910,
     "sourceId": 7467717,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 5786,
     "sourceId": 7366,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6290.945796,
   "end_time": "2024-01-29T12:51:33.168811",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-29T11:06:42.223015",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
