{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7467717,"sourceType":"datasetVersion","datasetId":4346910},{"sourceId":7366,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":5786}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-29T02:12:38.155472Z","iopub.execute_input":"2024-01-29T02:12:38.156239Z","iopub.status.idle":"2024-01-29T02:12:38.173849Z","shell.execute_reply.started":"2024-01-29T02:12:38.156203Z","shell.execute_reply":"2024-01-29T02:12:38.172271Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-01-29T02:12:40.780375Z","iopub.execute_input":"2024-01-29T02:12:40.781180Z","iopub.status.idle":"2024-01-29T02:12:41.759879Z","shell.execute_reply.started":"2024-01-29T02:12:40.781057Z","shell.execute_reply":"2024-01-29T02:12:41.758735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar xvfz /kaggle/input/bert4keras-packages/packs.tgz","metadata":{"execution":{"iopub.status.busy":"2024-01-29T02:12:49.926363Z","iopub.execute_input":"2024-01-29T02:12:49.926742Z","iopub.status.idle":"2024-01-29T02:12:56.994726Z","shell.execute_reply.started":"2024-01-29T02:12:49.926710Z","shell.execute_reply":"2024-01-29T02:12:56.993705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert4keras --no-deps --force --no-index --find-links=file:///kaggle/working/packages/\n!pip install tensorboard_data_server tensorboard Keras_Preprocessing tf_estimator_nightly tensorboard_plugin_wit --no-deps --force --no-index --find-links=file:///kaggle/working/packages/\n!pip install tensorflow keras --no-deps --force --no-index --find-links=file:///kaggle/working/packages/","metadata":{"execution":{"iopub.status.busy":"2024-01-29T02:13:00.834308Z","iopub.execute_input":"2024-01-29T02:13:00.834723Z","iopub.status.idle":"2024-01-29T02:13:47.485552Z","shell.execute_reply.started":"2024-01-29T02:13:00.834689Z","shell.execute_reply":"2024-01-29T02:13:47.484446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial env","metadata":{}},{"cell_type":"code","source":"#!pip install bert4keras --no-deps --force\n#!pip install tensorflow-gpu==2.8.0\n#!pip install tensorflow==2.8.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /opt/conda/lib/python3.10/site-packages/bert4keras/backend.py _b.py\n!sed -i \"18s/sys/#sys/\" _b.py\n!sed \"18a\\    from tensorflow import keras\\n    import tensorflow.keras.backend as K\" _b.py > _b2.py\n!sed -i \"22s/import/#import/\" _b2.py\n!sed -i \"23s/import/#import/\" _b2.py\n!cp _b2.py /opt/conda/lib/python3.10/site-packages/bert4keras/backend.py","metadata":{"execution":{"iopub.status.busy":"2024-01-29T02:13:54.297888Z","iopub.execute_input":"2024-01-29T02:13:54.298701Z","iopub.status.idle":"2024-01-29T02:13:59.919389Z","shell.execute_reply.started":"2024-01-29T02:13:54.298663Z","shell.execute_reply":"2024-01-29T02:13:59.918301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!curl -LJO https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip\n#!unzip uncased_L-12_H-768_A-12.zip -d bert_base\n\n#!curl -LJO https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n#!unzip uncased_L-12_H-768_A-12.zip -d bert_large","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert raw data to train set","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport json\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\ntrain_file = '/kaggle/input/pii-detection-removal-from-educational-data/train.json'\ntest_file = '/kaggle/input/pii-detection-removal-from-educational-data/test.json'\n\nsplit_ratio = 0.8\n\n\ndef __convert(indata, include_blank=True):\n\n    text = ''\n    entities = []\n    all_idx = 0\n    start_idx = 0\n    etype = ''\n\n    text = indata['sentence']\n\n    for n, label in enumerate(indata['BIO_label']):\n        if label[0]=='O':\n            if etype!='':\n                entities.append({\n                    \"start_idx\": len(''.join(text[:start_idx])),\n                    \"end_idx\": len((''.join(text[:all_idx])).rstrip()) - 1,\n                    \"type\": etype,\n                    \"entity\": (''.join(text[start_idx:all_idx])).rstrip(),\n                })\n            start_idx = 0\n            etype = ''\n        elif label[0]=='B':\n            if etype!='':\n                entities.append({\n                    \"start_idx\": len(''.join(text[:start_idx])),\n                    \"end_idx\": len((''.join(text[:all_idx])).rstrip()) - 1,\n                    \"type\": etype,\n                    \"entity\": (''.join(text[start_idx:all_idx])).rstrip(),\n                })                \n            start_idx = all_idx\n            etype = label.split('-')[1]\n        elif label[0]=='I':\n            pass\n        else:\n            print('unknown label: ', label)\n\n        all_idx += 1\n\n\n    # 一行text结束\n    if etype!='':\n        entities.append({\n            \"start_idx\": len(''.join(text[:start_idx])),\n            \"end_idx\": len((''.join(text[:all_idx])).rstrip()) - 1,\n            \"type\": etype,\n            \"entity\": (''.join(text[start_idx:all_idx])).rstrip(),\n        })\n\n    # 加入数据集\n    if include_blank or len(entities)>0:\n        return {\n            'text' : ''.join(text),\n            'entities' : entities,\n        }\n    else:\n        return None\n\n\n\n\ndef assemble(infile, outfile_path, max_len=500, is_train=True, include_blank=True):\n    total = text_break = tmp_break = 0\n    D = []\n\n    data = json.load(open(infile))\n    for l in tqdm(data):\n        #print(f\"---> {l['document']}\")\n\n        total += 1\n\n        text = []\n        tmp_text = []\n        n = n_text = n_tmp = 0\n        while n<len(l['tokens']):\n            token = l['tokens'][n].replace('…', '.').replace('´', \"'\").replace('²', '2')\\\n                .replace('΅', \"'\").replace('¨', \"'\").replace(';', ';').replace('．', '.')\\\n                .replace('³', '3').replace('‑', '-').replace('¹', '1').replace('½', '1')\\\n                .replace('¾', '1').replace('¼', '1').replace('\\xad', '-')\\\n                .replace('ﬄ', 'ffl').replace('ﬃ', 'ffi').replace('ﬂ', 'fl')\\\n                .replace('ﬁ', 'fi').replace('ﬀ', 'ff').replace('™', 'TM').replace('№', 'No')\n\n            if l['trailing_whitespace'][n]:\n                token += ' ' \n\n            if n_tmp + 1 > max_len:\n                text += deepcopy(tmp_text)\n                tmp_text = []\n                n_text += n_tmp\n                n_tmp = 0\n\n                tmp_break += 1\n\n                #print(text)\n                #assert False, f\"tmp_text is too long: {len(text)}, {len(tmp_text)}, {len(token)}\"\n\n            if n_text + n_tmp + 1 > max_len:\n                assert n_text>0, f\"too long: {n_text}, {n_tmp}, {max_len}\"\n                #print(text)\n                #print('-'*20)\n                dd = __convert({\n                        'sentence'  : [x[0] for x in text],\n                        'BIO_label' : [x[1] for x in text],\n                    }, include_blank=include_blank)\n                if dd:\n                    dd['document'] = l['document']\n                    if not is_train:\n                        dd['tokens'] = [x[0] for x in text]\n                    D.append(dd)\n                text = []\n                n_text = 0\n\n                text_break += 1\n\n            if is_train:\n                tmp_text += [(token, l['labels'][n])] # token, label\n            else:\n                tmp_text += [(token, 'O')] # token, blank-label\n            n_tmp += 1\n\n            if token=='\\n\\n':\n                text += deepcopy(tmp_text)\n                tmp_text = []\n                n_text += n_tmp\n                n_tmp = 0\n\n            n += 1\n\n        if n_text + n_tmp > 0:\n            text += deepcopy(tmp_text)\n            n_text += n_tmp\n            #print(text)\n            #print('-'*20)\n            dd = __convert({\n                    'sentence'  : [x[0] for x in text],\n                    'BIO_label' : [x[1] for x in text],\n                }, include_blank=include_blank)\n            if dd:\n                dd['document'] = l['document']\n                if not is_train:\n                    dd['tokens'] = [x[0] for x in text]\n                D.append(dd)\n\n            text_break += 1            \n\n        #break # for test\n\n    blank = 0\n    for x in D:\n        if len(x['entities'])==0:\n            blank += 1\n\n    print(f\"total= {total}\\ttext_break= {text_break}\\ttmp_break= {tmp_break}\\tblank= {blank}\")\n\n    if is_train:\n        random.shuffle(D)\n\n        # 拆分数据集\n        split_n = int(len(D) * split_ratio)\n\n        json.dump(\n            D[:split_n],\n            open(os.path.join(outfile_path, \"train.json\"), 'w', encoding='utf-8'),\n            indent=4,\n            ensure_ascii=False\n        )\n\n\n        json.dump(\n            D[split_n:],\n            open(os.path.join(outfile_path, \"dev.json\"), 'w', encoding='utf-8'),\n            indent=4,\n            ensure_ascii=False\n        )\n\n        print(f\"train set: {split_n}\\tdev set: {len(D)-split_n}\")\n\n    else:\n        json.dump(\n            D,\n            open(os.path.join(outfile_path, \"test.json\"), 'w', encoding='utf-8'),\n            indent=4,\n            ensure_ascii=False\n        )\n\n        print(f\"test set: {len(D)}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-29T02:14:02.623802Z","iopub.execute_input":"2024-01-29T02:14:02.624186Z","iopub.status.idle":"2024-01-29T02:14:02.663231Z","shell.execute_reply.started":"2024-01-29T02:14:02.624155Z","shell.execute_reply":"2024-01-29T02:14:02.662357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assemble(train_file, '.', include_blank=False)\nassemble(test_file, '.', is_train=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T02:18:10.576969Z","iopub.execute_input":"2024-01-29T02:18:10.577781Z","iopub.status.idle":"2024-01-29T02:18:52.780542Z","shell.execute_reply.started":"2024-01-29T02:18:10.577745Z","shell.execute_reply":"2024-01-29T02:18:52.779577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"#! -*- coding: utf-8 -*-\n# 用GlobalPointer做中文命名实体识别\n\nimport os\nos.environ[\"TF_KERAS\"] = \"1\" # use tf 2.7 keras\n\nimport json\nimport math\nimport numpy as np\nfrom bert4keras.backend import keras, K\nfrom bert4keras.backend import multilabel_categorical_crossentropy\n#from bert4keras.layers import GlobalPointer\nfrom bert4keras.layers import EfficientGlobalPointer as GlobalPointer\nfrom bert4keras.models import build_transformer_model\nfrom bert4keras.tokenizers import Tokenizer\nfrom bert4keras.optimizers import Adam, extend_with_exponential_moving_average\nfrom bert4keras.snippets import sequence_padding, DataGenerator\nfrom bert4keras.snippets import open, to_array\nfrom keras.models import Model\nfrom keras.callbacks import LearningRateScheduler\nfrom tqdm import tqdm\n\nmaxlen = 512\nepochs = 10\nbatch_size = 4\nlearning_rate = 1e-5\ncategories = set()\n\n# bert配置\nconfig_path = '/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/bert_config.json'\ncheckpoint_path = None #'/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/bert_model.ckpt'\ndict_path = '/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/vocab.txt'\n\ndef load_data(filename):\n    \"\"\"加载数据\n    单条格式：[text, (start, end, label), (start, end, label), ...]，\n              意味着text[start:end + 1]是类型为label的实体。\n    \"\"\"\n    D = []\n    for d in json.load(open(filename)):\n        D.append([d['text']])\n        for e in d['entities']:\n            start, end, label = e['start_idx'], e['end_idx'], e['type']\n            if start <= end:\n                D[-1].append((start, end, label))\n            categories.add(label)\n    return D\n\n\n# 标注数据\ntrain_data = load_data('./train.json')\nvalid_data = load_data('./dev.json')\ncategories = list(sorted(categories))\n\nprint(\"labels: \", categories)\n# labels:  ['EMAIL', 'ID_NUM', 'NAME_STUDENT', 'PHONE_NUM', 'STREET_ADDRESS', 'URL_PERSONAL', 'USERNAME']\n\n\n# 建立分词器\ntokenizer = Tokenizer(dict_path, do_lower_case=True)\n\n\nclass data_generator(DataGenerator):\n    \"\"\"数据生成器\n    \"\"\"\n    def __iter__(self, random=False):\n        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n        for is_end, d in self.sample(random):\n            tokens = tokenizer.tokenize(d[0], maxlen=maxlen)\n            mapping = tokenizer.rematch(d[0], tokens)\n            start_mapping = {j[0]: i for i, j in enumerate(mapping) if j}\n            end_mapping = {j[-1]: i for i, j in enumerate(mapping) if j}\n            token_ids = tokenizer.tokens_to_ids(tokens)\n            segment_ids = [0] * len(token_ids)\n            labels = np.zeros((len(categories), maxlen, maxlen))\n            for start, end, label in d[1:]:\n                if start in start_mapping and end in end_mapping:\n                    start = start_mapping[start]\n                    end = end_mapping[end]\n                    label = categories.index(label)\n                    labels[label, start, end] = 1\n            batch_token_ids.append(token_ids)\n            batch_segment_ids.append(segment_ids)\n            batch_labels.append(labels[:, :len(token_ids), :len(token_ids)])\n            if len(batch_token_ids) == self.batch_size or is_end:\n                batch_token_ids = sequence_padding(batch_token_ids)\n                batch_segment_ids = sequence_padding(batch_segment_ids)\n                batch_labels = sequence_padding(batch_labels, seq_dims=3)\n                yield [batch_token_ids, batch_segment_ids], batch_labels\n                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n\n\ndef global_pointer_crossentropy(y_true, y_pred):\n    \"\"\"给GlobalPointer设计的交叉熵\n    \"\"\"\n    bh = K.prod(K.shape(y_pred)[:2])\n    y_true = K.reshape(y_true, (bh, -1))\n    y_pred = K.reshape(y_pred, (bh, -1))\n    return K.mean(multilabel_categorical_crossentropy(y_true, y_pred))\n\n\ndef global_pointer_f1_score(y_true, y_pred, epsilon=1e-10):\n    \"\"\"给GlobalPointer设计的F1\n    \"\"\"\n    y_pred = K.cast(K.greater(y_pred, 0), K.floatx())\n    return 2 * K.sum(y_true * y_pred) / (K.sum(y_true + y_pred) + epsilon)\n\n\nmodel = build_transformer_model(config_path, checkpoint_path)\noutput = GlobalPointer(len(categories), 64)(model.output)\n\nAdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\noptimizer = AdamEMA(learning_rate=learning_rate)\n\nmodel = Model(model.input, output)\nmodel.summary()\n\nmodel.compile(\n    loss=global_pointer_crossentropy,\n    optimizer=optimizer, #Adam(learning_rate),\n    #optimizer=Adam(learning_rate),\n    metrics=[global_pointer_f1_score]\n)\n\n\nclass NamedEntityRecognizer(object):\n    \"\"\"命名实体识别器\n    \"\"\"\n    def recognize(self, text, threshold=0):\n        tokens = tokenizer.tokenize(text, maxlen=512)\n        mapping = tokenizer.rematch(text, tokens)\n        token_ids = tokenizer.tokens_to_ids(tokens)\n        segment_ids = [0] * len(token_ids)\n        token_ids, segment_ids = to_array([token_ids], [segment_ids])\n        scores = model.predict([token_ids, segment_ids])[0]\n        scores[:, [0, -1]] -= np.inf\n        scores[:, :, [0, -1]] -= np.inf\n        entities = []\n        for l, start, end in zip(*np.where(scores > threshold)):\n            entities.append(\n                (mapping[start][0], mapping[end][-1], categories[l])\n            )\n        return entities\n\n\nNER = NamedEntityRecognizer()\n\n\ndef evaluate(data):\n    \"\"\"评测函数\n    \"\"\"\n    X, Y, Z = 1e-10, 1e-10, 1e-10\n    for d in tqdm(data, ncols=100):\n        R = set(NER.recognize(d[0]))\n        T = set([tuple(i) for i in d[1:]])\n        X += len(R & T)\n        Y += len(R)\n        Z += len(T)\n    f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n    return f1, precision, recall\n\n\nclass Evaluator(keras.callbacks.Callback):\n    \"\"\"评估与保存\n    \"\"\"\n    def __init__(self):\n        self.best_val_f1 = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        f1, precision, recall = evaluate(valid_data)\n        # 保存最优\n        if f1 >= self.best_val_f1:\n            self.best_val_f1 = f1\n            model.save_weights('./pii_gp_best_f1.h5')\n        print(\n            'valid:  f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n            (f1, precision, recall, self.best_val_f1)\n        )\n\n\ndef predict_to_file(in_file, out_file):\n    \"\"\"预测到文件\n    \"\"\"\n    data = json.load(open(in_file))\n\n    document = \"\"\n    last_pos = 0\n    D = []\n\n    for d in tqdm(data, ncols=100):\n\n        if document != d['document']:\n            document = d['document']\n            last_pos = 0\n\n        # 初始化 BIO 标记\n        label = ['O']*len(d['tokens'])\n\n        # 识别\n        entities = NER.recognize(d['text'])\n        for e in entities:\n            d['entities'].append({\n                'start_idx': e[0],\n                'end_idx': e[1],\n                'type': e[2]\n            })\n\n            # 生成 BIO标记, 依据 原始 tokens\n            pos = last = 0\n            for n, x in enumerate(d['tokens']):\n                if pos >= e[0] and pos <= e[1]+1:\n                    if last==0:\n                        label[n] = 'B-'+e[2]\n                        last += 1\n                    else:\n                        label[n] = 'I-'+e[2]\n                    D.append((document, last_pos+n, label[n]))\n                else:\n                    last = 0\n\n                pos += len(x)\n\n                if pos > e[1]:\n                    break\n\n        d['labels'] = label\n\n        last_pos += len(d['tokens'])\n\n    # 保存json格式\n    json.dump(\n        data,\n        open(\"output.json\", 'w', encoding='utf-8'),\n        indent=4,\n        ensure_ascii=False\n    )\n\n    with open(out_file, \"w\") as f:\n        f.write(\"row_id,document,token,label\\n\")\n        for n, x in enumerate(D):\n            f.write(f\"{n},{x[0]},{x[1]},{x[2]}\\n\")\n\ndef lr_step_decay(epoch):\n    drop = 0.8\n    epochs_drop = 1.0\n    lrate = learning_rate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n    return lrate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# 训练\nbatch_size = 2\nepochs = 10\n\nevaluator = Evaluator()\nlrate = LearningRateScheduler(lr_step_decay)\ntrain_generator = data_generator(train_data, batch_size)\n\nmodel.load_weights('/kaggle/input/google_bert/tensorflow2/bert_large_uncased_pii_43k_noblank/1/pii_gp_best_f1_0.92609.h5')\n\nmodel.fit(\n    train_generator.forfit(),\n    steps_per_epoch=len(train_generator),\n    epochs=epochs,\n    callbacks=[evaluator, lrate]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate result","metadata":{}},{"cell_type":"code","source":"# 推理生成结果\n\nmodel.load_weights('pii_gp_best_f1.h5')\npredict_to_file('test.json', 'submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-25T13:22:11.327124Z","iopub.execute_input":"2024-01-25T13:22:11.327989Z","iopub.status.idle":"2024-01-25T13:22:11.931460Z","shell.execute_reply.started":"2024-01-25T13:22:11.327953Z","shell.execute_reply":"2024-01-25T13:22:11.930491Z"},"trusted":true},"execution_count":null,"outputs":[]}]}